etl:
  # MATRIX CONFIGURATION
  # For quick testing in Streamlit, we default to a smaller subset.
  # Uncomment options to run full benchmark.
  libraries: [ 'pandas', 'polars' ] # Available: [ 'pandas', 'polars', 'duckdb', 'dask' ]
  sources: [ 'csv' ] # Available: [ 'csv', 'parquet', 'json' ]
  destinations: [ 'csv' ] # Available: [ 'csv', 'parquet', 'json' ]
  
  rows_limit: 1000 # Increase for stress testing (e.g., 100000)

  # BASE PATHS
  base_input_path: '/home/fhp101ml/Documentos/Proyectos/Personales/ETLPerformanace/data_test/complex_input' 
  output_base_dir: '/home/fhp101ml/Documentos/Proyectos/Personales/ETLPerformanace/data_test/matrix_output'

  # EXTRACTOR CONFIG (Applied dynamically based on current source)
  extractor_options:
    csv:
      separator: ';'
    json: {}
    parquet: {}

  # EXTERNAL DATA
  external_data:
    municipalities_path: '/home/fhp101ml/Documentos/Proyectos/Personales/ETLPerformanace/data_test/external/spanish_municipalities.csv'

  # TRANSFORMATIONS (Applied to all)
  transformations:
    general: []
    attributes:
        name: [ 'capitalize_first_letter' ]
        salary: [ 'impute_mean' ]
        last_login: [ 'days_since' ]

metadata:
  attributes: ['id', 'name', 'birth_date', 'email', 'zip_code', 'current_lat', 'current_lon', 'salary', 'category', 'last_login']
  types:
    id: int
    name: str
    salary: float
    last_login: datetime
  rules:
    name: capitalize_first_letter
    salary: impute_mean
    last_login: days_since
